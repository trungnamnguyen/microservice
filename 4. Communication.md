There are lots of different IPC technologies to choose from. Services can use synchronous request/response-based communication mechanisms, such as HTTP-based REST or gRPC. Alternatively, they can use asynchronous, message-based communication mechanisms such as AMQP or STOMP. There are also a variety of different messages formats. Services can use human-readable, text-based formats such as JSON or XML. Alternatively, they could use a more efficient binary format such as Avro or Protocol Buffers.
# 1. Overview of interprocess communication in a microservice architecture 
# 1.1. Interaction styles
#### They can be categorized in two dimensions
The first dimension is whether the interaction is one-to-one or one-to-many:
- One-to-one—Each client request is processed by exactly one service.
- One-to-many—Each request is processed by multiple services.

The second dimension is whether the interaction is synchronous or asynchronous:

- Synchronous—The client expects a timely response from the service and might even block while it waits.
- Asynchronous—The client doesn’t block, and the response, if any, isn’t necessarily sent immediately.

# 1.2. Defining APIs in a microservice architecture
- APIs or interfaces are central to software development. An application is comprised of modules. Each module has an interface that defines the set of operations that module’s clients can invoke
- The challenge is that a service API isn’t defined using a simple programming language construct. By definition, a service and its clients aren’t compiled together. If a new version of a service is deployed with an incompatible API, there’s no compilation error. Instead, there will be runtime failures.
- Regardless of which IPC mechanism you choose, it’s important to precisely define a service’s API using some kind of interface definition language (IDL)
# 1.3. Evolving APIs
In a microservices-based application, changing a service’s API is a lot more difficult. A service’s clients are other services, which are often developed by other teams. The clients may even be other applications outside of the organization. You usually can’t force all clients to upgrade in lockstep with the service

Strategy:
- **USE SEMANTIC VERSIONING:** The Semantic Versioning specification (http://semver.org)

The Semantic Versioning specification (Semvers) requires a version number to consist of three parts: MAJOR.MINOR.PATCH. You must increment each part of a version number as follows:

**MAJOR**—When you make an incompatible change to the API

**MINOR**—When you make backward-compatible enhancements to the API

**PATCH**—When you make a backward-compatible bug fix

- **MAKING MINOR, BACKWARD-COMPATIBLE CHANGES**

Ideally, you should strive to only make backward-compatible changes. Backward-compatible changes are additive changes to an API:

Adding optional attributes to request
Adding attributes to a response
Adding new operations
# 1.4. Message formats

There are two main categories of message formats: text and binary

**TEXT-BASED MESSAGE FORMATS:**
- The first category is text-based formats such as JSON and XML.An advantage of these formats is that not only are they human readable, they’re self describing. A downside of using a text-based messages format is that the messages tend to be verbose, especially XML. Every message has the overhead of containing the names of the attributes in addition to their values. Another drawback is the overhead of parsing text, especially when messages are large. Consequently, if efficiency and performance are important, you may want to consider using a binary format.

**BINARY MESSAGE FORMATS:** Popular formats include Protocol Buffers  and Avro. Both formats provide a typed IDL for defining the structure of your messages. A compiler then generates the code that serializes and deserializes the messages
# 2. Communicating using the synchronous Remote procedure invocation pattern
# 2.1. Using REST
A key concept in REST is a resource

REST uses the HTTP verbs for manipulating resources, which are referenced using a URL

**THE REST MATURITY MODEL** 
Following levels:

- **Level 0**—Clients of a level 0 service invoke the service by making HTTP POST requests to its sole URL endpoint. Each request specifies the action to perform, the target of the action (for example, the business object), and any parameters.
- **Level 1**—A level 1 service supports the idea of resources. To perform an action on a resource, a client makes a POST request that specifies the action to perform and any parameters.
- **Level 2**—A level 2 service uses HTTP verbs to perform actions: GET to retrieve, POST to create, and PUT to update. The request query parameters and body, if any, specify the actions’ parameters. This enables services to use web infrastructure such as caching for GET requests.
- **Level 3**—The design of a level 3 service is based on the terribly named HATEOAS (Hypertext As The Engine Of Application State) principle. The basic idea is that the representation of a resource returned by a GET request contains links for performing actions on that resource. For example, a client can cancel an order using a link in the representation returned by the GET request that retrieved the order. The benefits of HATEOAS include no longer having to hard-wire URLs into client code.

**BENEFITS AND DRAWBACKS OF REST**

There are numerous benefits to using REST:

- It’s simple and familiar.
- You can test an HTTP API from within a browser using, for example, the Postman plugin, or from the command line using curl (assuming JSON or some other text format is used).
- It directly supports request/response style communication.
- HTTP is, of course, firewall friendly.
- It doesn’t require an intermediate broker, which simplifies the system’s architecture.

There are some drawbacks to using REST:

- It only supports the request/response style of communication.
- Reduced availability. Because the client and service communicate directly without an intermediary to buffer messages, they must both be running for the duration of the exchange.
- Clients must know the locations (URLs) of the service instances(s). As described in section 3.2.4, this is a nontrivial problem in a modern application. Clients must use what is known as a service discovery mechanism to locate service instances.
- Fetching multiple resources in a single request is challenging.
- It’s sometimes difficult to map multiple update operations to HTTP verbs.

# 2.2. Using gRPC
gRPC is a binary message-based protocol.

You define your gRPC APIs using a Protocol Buffers-based IDL, which is Google’s language-neutral mechanism for serializing structured data.

You use the Protocol Buffer compiler to generate client-side stubs and server-side skeletons.

Clients and servers exchange binary messages in the Protocol Buffers format using HTTP/2.

gRPC has several benefits:

- It’s straightforward to design an API that has a rich set of update operations.
- It has an efficient, compact IPC mechanism, especially when exchanging large messages.
- Bidirectional streaming enables both RPI and messaging styles of communication.
- It enables interoperability between clients and services written in a wide range of languages.

gRPC also has several drawbacks:

It takes more work for JavaScript clients to consume gRPC-based API than REST/JSON-based APIs.
Older firewalls might not support HTTP/2.

# 2.3. Handling partial failure using the Circuit breaker pattern

In a distributed system, whenever a service makes a synchronous request to another service, there is an ever-present risk of partial failure

Whenever one service synchronously invokes another service, it should protect itself:

- **Network timeouts**—Never block indefinitely and always use timeouts when waiting for a response. Using timeouts ensures that resources are never tied up indefinitely.
- **Limiting the number of outstanding requests from a client to a service**—Impose an upper bound on the number of outstanding requests that a client can make to a particular service. If the limit has been reached, it’s probably pointless to make additional requests, and those attempts should fail immediately.
- **Circuit breaker pattern**—Track the number of successful and failed requests, and if the error rate exceeds some threshold, trip the circuit breaker so that further attempts fail immediately. A large number of requests failing suggests that the service is unavailable and that sending more requests is pointless. After a timeout period, the client should try again, and, if successful, close the circuit breaker.

# 2.4. Using service discovery

Service discovery is conceptually quite simple: its key component is a service registry, which is a database of the network locations of an application’s service instances.

There are two main ways to implement service discovery:

- The services and their clients interact directly with the service registry.
- The deployment infrastructure handles service discovery.

**APPLYING THE APPLICATION-LEVEL SERVICE DISCOVERY PATTERNS**

![servicediscovery.JPG](assets/servicediscovery.JPG?t=1700581056703)

This approach to service discovery is a combination of two patterns:

- Pattern: Self registration

A service instance registers itself with the service registry.

- Pattern: Client-side discovery

A service client retrieves the list of available service instances from the service registry and load balances across them.

Application-level service discovery has been popularized by Netflix and Pivotal. Netflix developed and open sourced several components: Eureka, a highly available service registry, the Eureka Java client, and Ribbon, a sophisticated HTTP client that supports the Eureka client. Pivotal developed Spring Cloud, a Spring-based framework that makes it remarkably easy to use the Netflix components. Spring Cloud-based services automatically register with Eureka, and Spring Cloud-based clients automatically use Eureka for service discovery.

One benefit of application-level service discovery is that it handles the scenario when services are deployed on multiple deployment platform.

One drawback of application-level service discovery is that you need a service discovery library for every language—and possibly framework—that you use

**APPLYING THE PLATFORM-PROVIDED SERVICE DISCOVERY PATTERNS**

![servicediscovery1.JPG](assets/servicediscovery1.JPG?t=1700581056703)

This approach is a combination of two patterns:

- **3rd party registration pattern**—Instead of a service registering itself with the service registry, a third party called the registrar, which is typically part of the deployment platform, handles the registration.
- **Server-side discovery pattern**—Instead of a client querying the service registry, it makes a request to a DNS name, which resolves to a request router that queries the service registry and load balances requests.

- Pattern: 3rd party registration: Service instances are automatically registered with the service registry by a third party
- Pattern: Server-side discovery: A client makes a request to a router, which is responsible for service discovery.

# 3. Communicating using the Asynchronous messaging pattern

# 3.1. Overview of messaging

A sender (an application or service) writes a message to a channel, and a receiver (an application or service) reads messages from a channel.

There are several different kinds of messages:

- **Document**—A generic message that contains only data. The receiver decides how to interpret it. The reply to a command is an example of a document message.
- **Command**—A message that’s the equivalent of an RPC request. It specifies the operation to invoke and its parameters.
- **Event**—A message indicating that something notable has occurred in the sender. An event is often a domain event, which represents a state change of a domain object such as an Order, or a Customer.

There are two kinds of channels:

- A point-to-point channel delivers a message to exactly one of the consumers that is reading from the channel. Services use point-to-point channels for the one-to-one interaction styles described earlier. For example, a command message is often sent over a point-to-point channel.
- A publish-subscribe channel delivers each message to all of the attached consumers. Services use publish-subscribe channels for the one-to-many interaction styles described earlier. For example, an event message is usually sent over a publish-subscribe channel.
# 3.2. Implementing the interaction styles using messaging

**IMPLEMENTING REQUEST/RESPONSE AND ASYNCHRONOUS REQUEST/RESPONSE**

![communication1.JPG](assets/communication1.JPG?t=1700581056703)

https://reflectoring.io/amqp-request-response/

**IMPLEMENTING ONE-WAY NOTIFICATIONS**

Implementing one-way notifications is straightforward using asynchronous messaging. The client sends a message, typically a command message, to a point-to-point channel owned by the service. The service subscribes to the channel and processes the message. It doesn’t send back a reply.

**IMPLEMENTING PUBLISH/SUBSCRIBE**

**IMPLEMENTING PUBLISH/ASYNC RESPONSES**

The publish/async responses interaction style is a higher-level style of interaction that’s implemented by combining elements of publish/subscribe and request/response

A client publishes a message that specifies a reply channel header to a publish-subscribe channel. A consumer writes a reply message containing a correlation id to the reply channel. The client gathers the responses by using the correlation id to match the reply messages with the request.

# 3.3. Using message broker

![brokerarchitech.JPG](assets/brokerarchitech.JPG?t=1700581056703)

**BROKERLESS MESSAGING**

In a brokerless architecture, services can exchange messages directly. ZeroMQ (http://zeromq.org) is a popular brokerless messaging technology. It’s both a specification and a set of libraries for different languages. It supports a variety of transports, including TCP, UNIX-style domain sockets, and multicast.

The brokerless architecture has some benefits:

- Allows lighter network traffic and better latency, because messages go directly from the sender to the receiver, instead of having to go from the sender to the message broker and from there to the receiver
- Eliminates the possibility of the message broker being a performance bottleneck or a single point of failure
- Features less operational complexity, because there is no message broker to set up and maintain
As appealing as these benefits may seem, brokerless messaging has significant drawbacks:

- Services need to know about each other’s locations and must therefore use one of the discovery mechanisms.
- It offers reduced availability, because both the sender and receiver of a message must be available while the message is being exchanged.
- Implementing mechanisms, such as guaranteed delivery, is more challenging.

**BROKER-BASED MESSAGING:**
A message broker is an intermediary through which all messages flow. A sender writes the message to the message broker, and the message broker delivers it to the receiver. An important benefit of using a message broker is that the sender doesn’t need to know the network location of the consumer. Another benefit is that a message broker buffers messages until the consumer is able to process them.

There are many message brokers to chose from. Examples of popular open source message brokers include the following:

- ActiveMQ (http://activemq.apache.org)
- RabbitMQ (https://www.rabbitmq.com)
- Apache Kafka (http://kafka.apache.org)
There are also cloud-based messaging services, such as AWS Kinesis (https://aws.amazon.com/kinesis/) and AWS SQS (https://aws.amazon.com/sqs/).

When selecting a message broker, you have various factors to consider, including the following:

- Supported programming languages—You probably should pick one that supports a variety of programming languages.
- Supported messaging standards—Does the message broker support any standards, such as AMQP and STOMP, or is it proprietary?
- Messaging ordering—Does the message broker preserve ordering of messages?
- Delivery guarantees—What kind of delivery guarantees does the broker make?
- Persistence—Are messages persisted to disk and able to survive broker crashes?
- Durability—If a consumer reconnects to the message broker, will it receive the messages that were sent while it was disconnected?
- Scalability—How scalable is the message broker?
- Latency—What is the end-to-end latency?
- Competing consumers—Does the message broker support competing consumers?

**BENEFITS AND DRAWBACKS OF BROKER-BASED MESSAGING**

There are many advantages to using broker-based messaging:

- Loose coupling—A client makes a request by simply sending a message to the appropriate channel. The client is completely unaware of the service instances. It doesn’t need to use a discovery mechanism to determine the location of a service instance.
- Message buffering—The message broker buffers messages until they can be processed. With a synchronous request/response protocol such as HTTP, both the client and service must be available for the duration of the exchange. With messaging, though, messages will queue up until they can be processed by the consumer. This means, for example, that an online store can accept orders from customers even when the order-fulfillment system is slow or unavailable. The messages will simply queue up until they can be processed.
- Flexible communication—Messaging supports all the interaction styles described earlier.
- Explicit interprocess communication—RPC-based mechanism attempts to make invoking a remote service look the same as calling a local service. But due to the laws of physics and the possibility of partial failure, they’re in fact quite different. Messaging makes these differences very explicit, so developers aren’t lulled into a false sense of security.
There are some downsides to using messaging:

- Potential performance bottleneck—There is a risk that the message broker could be a performance bottleneck. Fortunately, many modern message brokers are designed to be highly scalable.
- Potential single point of failure—It’s essential that the message broker is highly available—otherwise, system reliability will be impacted. Fortunately, most modern brokers have been designed to be highly available.
- Additional operational complexity—The messaging system is yet another system component that must be installed, configured, and operated.

**3.3. Transactional messaging**

**USING A DATABASE TABLE AS A MESSAGE QUEUE**

![tablemsgqueue.JPG](assets/tablemsgqueue.JPG?t=1700581056703)

- OUTBOX:  temporary message queue.
- MessageRelay: component that reads the OUTBOX table and publishes the messages to a message broker.

There are a couple of different ways to move messages from the database to the message broker

**PUBLISHING EVENTS BY USING THE POLLING PUBLISHER PATTERN**

If the application uses a relational database, a very simple way to publish the messages inserted into the OUTBOX table is for the MessageRelay to poll the table for unpublished messages. It periodically queries the table:

SELECT * FROM OUTBOX ORDERED BY ... ASC

Next, the MessageRelay publishes those messages to the message broker, sending one to its destination message channel. Finally, it deletes those messages from the OUTBOX table:

BEGIN
 DELETE FROM OUTBOX WHERE ID in (....)
COMMIT

=> Pattern: Polling publisher


**PUBLISHING EVENTS BY APPLYING THE TRANSACTION LOG TAILING PATTERN**

Pattern: Transaction log tailing

![logtailing.JPG](assets/logtailing.JPG?t=1700581056703)

There are a few examples of this approach in use:

- Debezium (http://debezium.io)—An open source project that publishes database changes to the Apache Kafka message broker.
- LinkedIn Databus (https://github.com/linkedin/databus)—An open source project that mines the Oracle transaction log and publishes the changes as events. LinkedIn uses Databus to synchronize various derived data stores with the system of record.
- DynamoDB streams (http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html)—DynamoDB streams contain the time-ordered sequence of changes (creates, updates, and deletes) made to the items in a DynamoDB table in the last 24 hours. An application can read those changes from the stream and, for example, publish them as events.
- Eventuate Tram (https://github.com/eventuate-tram/eventuate-tram-core)—Your author’s very own open source transaction messaging library that uses MySQL binlog protocol, Postgres WAL, or polling to read changes made to an OUTBOX table and publish them to Apache Kafka.

3.4. Using asynchronous messaging to improve availability

**USE ASYNCHRONOUS INTERACTION STYLES**

![asyncinstyle.JPG](assets/asyncinstyle.JPG?t=1700581056703)

**REPLICATE DATA**

![replicatedata.JPG](assets/replicatedata.JPG?t=1700581056703)

**FINISH PROCESSING AFTER RETURNING A RESPONSE**

![finishresponse.JPG](assets/finishresponse.JPG?t=1700581056703)

The sequence of events is as follows:
- Order Service creates an Order in a PENDING state.
- Order Service returns a response to its client containing the order ID.
- Order Service sends a ValidateConsumerInfo message to Consumer Service.
- Order Service sends a ValidateOrderDetails message to Restaurant Service.
- Consumer Service receives a ValidateConsumerInfo message, verifies the consumer can place an order, and sends a ConsumerValidated message to Order Service.
- Restaurant Service receives a ValidateOrderDetails message, verifies the menu item are valid and that the restaurant can deliver to the order’s delivery address, and sends an OrderDetailsValidated message to Order Service.
- Order Service receives ConsumerValidated and OrderDetailsValidated and changes the state of the order to VALIDATED.
...

